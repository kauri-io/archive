<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    <link rel="canonical" href="https://kauri.io/collections/A%20non%20practitioner%20guide%20to%20machine%20learning%20/regression-with-python-keras-and-tensorflow/">
    <link rel="shortcut icon" href="../../../img/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>Regression with Python, Keras and Tensorflow - kauri.io</title>
    <link href="../../../css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="../../../css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="../../../css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="../../../css/highlight.css">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="../../../js/jquery-3.2.1.min.js"></script>
    <script src="../../../js/bootstrap-3.3.7.min.js"></script>
    <script src="../../../js/highlight.pack.js"></script>
    
    <base target="_top">
    <script>
      var base_url = '../../..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "Regression with Python, Keras and Tensorflow", url: "#_top", children: [
          ]},
        ];

    </script>
    <script src="../../../js/base.js"></script>
    <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-172017815-1', 'kauri.io');
        ga('send', 'pageview');
    </script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
  <a name="_top"></a>
    

    <h1 id="regression-with-python-keras-and-tensorflow">Regression with Python, Keras and Tensorflow<a class="headerlink" href="#regression-with-python-keras-and-tensorflow" title="Permanent link"></a></h1>
<p><img alt="" src="https://ipfs.infura.io/ipfs/QmNWabGoWpGE821uheTAYrRkR5Koo8e7zsBMWUnw7gTNST" /></p>
<p>In this tutorial we are going to do a quick and dirty estimation of house prices based on a dataset from a Kaggle competition. Kaggle is the leading data science competition platform and provides a lot of datasets you can use to improve your skills.</p>
<p>For simplicity's sake, we will build a simple model to get us started and we will explore how to improve it in later articles. Before we start, download the following file, which contains the training dataset, the test dataset and a sample submission (in case you want to see how your model fares in comparison to others by submitting it to the competition on Kaggle)</p>
<p><a href="https://www.kaggle.com/c/5407/download-all">Download the dataset</a></p>
<p><a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview">Link to the competition</a></p>
<p>Start your Jupyter Notebook, create and name a new kernel and let's start by importing the dependencies that we'll need.</p>
<pre><code class="language-python">import pandas as pd
import numpy as np
import seaborn as sns
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation
print(tf.__version__)
</code></pre>
<p>Then, we need to import the dataset into our kernel, Pandas provides a handy <code>read_csv</code> method to import CSV files.</p>
<pre><code class="language-python">raw_dataset = pd.read_csv('./train.csv', skipinitialspace=True)
test_dataset = pd.read_csv('./test.csv', skipinitialspace=True)
</code></pre>
<p>and let's visualise the first few rows of both datasets with <code>raw_dataset.head()</code> and <code>test_dataset.head()</code>
<img alt="" src="https://ipfs.infura.io/ipfs/QmPHmJkQDmHrS7Cs7XTUey7q74KBFG4mZbicm61FZmYKjQ" /></p>
<p>As we can see, we have a lot of columns which we'll call features, of different types (you can run <code>raw_dataset.dtypes</code> to verify each columns data type), but for this tutorial we will focus on a small subset of features.</p>
<p>First let's extract our <code>SalePrice</code> column, which will be our label or dependent variable (the one we want to estimate) and display its distribution.</p>
<pre><code class="language-python">labels = raw_dataset['SalePrice']
sns.distplot(labels)
</code></pre>
<p><img alt="Skewed prices distribution chart" src="https://ipfs.infura.io/ipfs/QmZHSbHas8HmfqDgyLKrpqwHSvCj95w8DTFSuTsUu9DsnE" /></p>
<p>The prices distribution is heavily skewed towards the left and definitely not normally distributed, while we can train a model using the labels as they are, a more normally distributed input will make training easier.</p>
<pre><code class="language-python">labels = np.log1p(raw_dataset['SalePrice'])
sns.distplot(labels)
</code></pre>
<p><img alt="Corrected prices distribution" src="https://ipfs.infura.io/ipfs/Qmb9iuLirTD9iWSPhakbJ5VuaapmeKoZDkWMscL3vJkd5L" /></p>
<p>Much better now, let's just remember that our model will now estimate the log of the price, so we will need to convert it back by using <code>np.exp1()</code>.</p>
<p>We are now ready to filter out our datasets for the columns we are interested in:</p>
<pre><code class="language-python">train_data = raw_dataset[[
    'MoSold',
    'YrSold',
    'OverallCond',
    'OverallQual',
    'LotArea',
    'YearBuilt',
    'TotalBsmtSF',
    'GrLivArea',
    'GarageCars',
    'Neighborhood'
]]
test_data = test_dataset[[
    'MoSold',
    'YrSold',
    'OverallCond',
    'OverallQual',
    'LotArea',
    'YearBuilt',
    'TotalBsmtSF',
    'GrLivArea',
    'GarageCars',
    'Neighborhood']]
train_data.head()
</code></pre>
<p><img alt="Filtered datasets" src="https://ipfs.infura.io/ipfs/QmaHX4QqPXKYn1En9TgVmLmkKRQitLeDZ7C6Yn1zoRgAVu" /></p>
<p>Much more manageable! We now have a couple of problems. First, some of the numeric columns actually represent categories, like <code>GarageCars</code> or <code>OverallQual</code>. Secondly, our model will only accept numeric data, so we will need to convert our qualitative data into numbers. Let's first convert the first set to string.</p>
<pre><code class="language-python">train_data['MoSold'] = train_data['MoSold'].apply(str)
train_data['YrSold'] = train_data['YrSold'].apply(str)
train_data['OverallCond'] = train_data['OverallCond'].apply(str)
train_data['OverallQual'] = train_data['OverallQual'].apply(str)
## train_data['YearBuilt'].apply(str)
## train_data['GarageCars'].apply(str)
test_data['MoSold'] = test_data['MoSold'].apply(str)
test_data['YrSold'] = test_data['YrSold'].apply(str)
test_data['OverallCond'] = test_data['OverallCond'].apply(str)
test_data['OverallQual'] = test_data['OverallQual'].apply(str)
## test_data['YearBuilt'].apply(str)
## test_data['GarageCars'].apply(str)
train_data.dtypes
</code></pre>
<p>Ignore the warnings for now, as you can see we successfully migrated the columns in question are not integers anymore. For the second problems we are going to use a technique called OneHot, in which each value of a categorical column gets its own numeric column with either a 1 or a 0, depending if the columns match the original value.</p>
<pre><code class="language-python">one_hot_train = pd.get_dummies(train_data)
one_hot_test = pd.get_dummies(test_data)
</code></pre>
<p>Finally, we will need to address the same distribution problem we had with <code>SalePrice</code>, for example, if we plot <code>sns.distplot(one_hot_train['GrLivArea'])</code> we'll see a similar skew in the distribution. To do so, we could use the log of the value as we did before, but for the inputs we can use another technique. We'll extract the stats of each column and normalize the data based on the <code>mean</code> and <code>std</code> of each column.</p>
<pre><code class="language-python">stats = one_hot_train.describe().transpose()

def norm(x):
    return (x - stats['mean']) / stats['std']

normed_train = norm(one_hot_train)
normed_test = norm(one_hot_test)

normed_train.head()
</code></pre>
<p>Lastly we want to discard the normalized one hot columns, for a stronger input signal.</p>
<pre><code class="language-python">input_train = one_hot_train
input_train['LotArea'] = normed_train['LotArea']
input_train['TotalBsmtSF'] = normed_train['TotalBsmtSF']
input_train['GrLivArea'] = normed_train['GrLivArea']
input_train['GarageCars'] = normed_train['GarageCars']
input_train['YearBuilt'] = normed_train['YearBuilt']
input_test = one_hot_test
input_test['LotArea'] = normed_test['LotArea']
input_test['TotalBsmtSF'] = normed_test['TotalBsmtSF']
input_test['GrLivArea'] = normed_test['GrLivArea']
input_test['GarageCars'] = normed_test['GarageCars']
input_test['YearBuilt'] = normed_test['YearBuilt']
</code></pre>
<p>Our final input data should look like this:
<img alt="Final input data" src="https://ipfs.infura.io/ipfs/QmR3J77skdWhmcDU6pGGsMK8WV3W2GyQZt7hNuXWe2Qgcp" /></p>
<p>And let's save these datapoints to a pickle file, so we don't need to do all of this in case we want to reuse this data.</p>
<pre><code class="language-python">import pickle
pickle_out = open(f&quot;{ITERATION}labels.pickle&quot;,&quot;wb&quot;)
pickle.dump(labels, pickle_out)
pickle_out.close()

pickle_out = open(f&quot;{ITERATION}input_train.pickle&quot;,&quot;wb&quot;)
pickle.dump(input_train, pickle_out)
pickle_out.close()

pickle_out = open(f&quot;{ITERATION}input_test.pickle&quot;,&quot;wb&quot;)
pickle.dump(input_test, pickle_out)
pickle_out.close()
</code></pre>
<p>You can later access the data using</p>
<pre><code class="language-python">import pickle
pickle_in = open(&quot;../input/house-prices-pickles-1/1.labels.pickle&quot;,&quot;rb&quot;)
labels = pickle.load(pickle_in)
</code></pre>
<p>Time to build our model and train it!</p>
<pre><code class="language-python">model = Sequential()

model.add(Dense(32, input_shape=input_train.shape[1:]))
model.add(Activation('sigmoid'))
model.add(Dense(1))
model.add(Activation('relu'))

model.compile(
    loss='mean_squared_error',
    optimizer='adam',
    metrics=['mean_squared_error','mean_absolute_error']
)

model.fit(
    input_train,
    labels,
    batch_size=32,
    epochs=30,
    validation_split=0.1,
    verbose=1
)
</code></pre>
<p>For each Epoch, you'll see some stats, as we did input the log of the price we'll want to focus on the <code>mean_absolute_error</code>. After 30 epochs, it will be around <code>0.135</code>, that means that for each prediction we should be in the range of ±0.135 from the log of the price in question. For a $500,000 we could calculate it like so:</p>
<pre><code class="language-python">logged_price = np.log(500000) # 13.122363377404328
lower_boundary = np.exp(logged_price - 0.135) # 436857.95584401704
upper_boundary = np.exp(logged_price + 0.135) # 572268.3921756567 
</code></pre>
<p>That's around 13% off, not perfect, but not bad either. The score is calculated on a small subset of the input data which we have defined with our <code>validation_split</code> parameter.</p>
<p>It is now time to generate some results on our test_dataset!</p>
<pre><code class="language-python">predictions = np.exp(model.predict(input_test))
sns.distplot(predictions)
</code></pre>
<p>Unfortunately, we won't be able to render the chart, as our model wasn't able to estimate a few values, a reasonable approach, for now would be to just replace them with the mean of the dataset.</p>
<pre><code class="language-python">predictions = np.exp(model.predict(input_test))
test_dataset['SalePrice'] = predictions
results = test_dataset[['Id','SalePrice']]
results = results.fillna(np.exp(labels.describe()['mean']))
results.isna().sum()
results.head()
</code></pre>
<p>and finally let's render the two distribution plots for a quick eye check on how our model works :)</p>
<pre><code class="language-python">sns.distplot(results['SalePrice'])
sns.distplot(np.exp(labels))
</code></pre>
<p><img alt="Fully rendered model" src="https://ipfs.infura.io/ipfs/QmXMJ8MquB61jor9Y6G4WyLWmQJudE719ndWVMgBaNUrK1" /></p>
<p>That's it! You've built your first models for estimating the price of real estate property! This model clearly needs some work but we'll cover it in the following articles. If you want to get ahead, try tweaking some of the parameters, like increasing the number of Epochs, pre-processing the data a bit differently or the structure of the models and see if you can improve the model yourself.</p>
<p>Also feel free to join the competition on Kaggle and see how your model fairs against fellow data nerds!
If you have any question or spot any error please feel free to comment or submit and update to this article :)</p>
<hr />
<ul>
<li><strong>Kauri original title:</strong> Regression with Python, Keras and Tensorflow</li>
<li><strong>Kauri original link:</strong> https://kauri.io/regression-with-python-keras-and-tensorflow/d035bc33cd37467db92e5b428a7565fd/a</li>
<li><strong>Kauri original author:</strong> Davide Scalzo (@davidescalzo)</li>
<li><strong>Kauri original Publication date:</strong> 2019-10-22</li>
<li><strong>Kauri original tags:</strong> artificial-intelligence, machine-learning, regression</li>
<li><strong>Kauri original hash:</strong> QmXrSb83unEwNB2Lv5R8VME38ctmjRoLE5W1ipH5uQPa1b</li>
<li><strong>Kauri original checkpoint:</strong> QmYRYAA1TRyDiXS6uLXdt6qS8AnW63tqJHYpUQKrdyNz7h</li>
</ul>

  <br>
    

    
    
      
    

  <div class="row wm-article-nav-buttons" role="navigation" aria-label="navigation">
    
    <div class="wm-article-nav pull-right">
      <a href="../predict-cryptocurrency-prices-with-tensorflow-as-/" class="btn btn-xs btn-default pull-right">
        Next
        <i class="fa fa-chevron-right" aria-hidden="true"></i>
      </a>
      <a href="../predict-cryptocurrency-prices-with-tensorflow-as-/" class="btn btn-xs btn-link">
        Predict cryptocurrency prices with Tensorflow as binary classification problem
      </a>
    </div>
    
    <div class="wm-article-nav">
      <a href="../installing-anaconda-python3-and-tensorflow/" class="btn btn-xs btn-default pull-left">
        <i class="fa fa-chevron-left" aria-hidden="true"></i>
        Previous</a><a href="../installing-anaconda-python3-and-tensorflow/" class="btn btn-xs btn-link">
        Installing Anaconda, Python3 and Tensorflow
      </a>
    </div>
    
  </div>

    <br>
</div>

<footer class="container-fluid wm-page-content">
      <p>
        <a href="https://github.com/kauri-io/archive/edit/main/docs/collections/A non practitioner guide to machine learning /regression-with-python-keras-and-tensorflow.md"><i class="fa fa-github"></i>
Edit on GitHub</a>
      </p>
  <p>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a> using <a href="https://github.com/gristlabs/mkdocs-windmill">Windmill</a> theme by Grist Labs.</p>
</footer>

</body>
</html>